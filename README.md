# Machine Translation

A sequence-to-sequence Transformer applied to a machine translation task. Source and target languages are English and Spanish respectivley. 

## Description

A sequence-to-sequence Transformer model was built for Engilsh-to-Spanish machine translation. The Transformer architecture was built from three subcomponents: Encoder, Decoder and Positional Embedding. The dataset is available at https://www.manythings.org/anki/, where ~130k samples were taken from the Tatoeba Project. Word frequency distributions were generated for both the source and target languages. Text data was vectorized and formatted for model training. Test English  sentences were set aside to examine how well translation is done on unseen data samples. Users can also input their own English sentences and be outputted Spanish sentence inferences. Intution behind Transformers and the role in machine translation can be found here: http://peterbloem.nl/blog/transformers https://www.tensorflow.org/text/tutorials/transformer

## Getting Started

### Dependencies

### Installing

### Executing program

## Help

## Authors

## Acknowledgments
